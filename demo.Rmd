---
title: "Sparse PLS-PM"
output: html_notebook
---


Structural Equation Modeling (SEM)

- PLS-PM is the Partial Least Squares approach to Structural Equation Modeling.
- PLS-PM is a statistical method for studying complex multivariate relationships 
among observed and latent variables.
- PLS-PM is a data analysis approach for studying a set of blocks of observed variables in which each block can be summarized by a latent variable and that linear relations exist between latent variables.


My preferred description is the one that views PLS-PM from a broader conceptual perspective for analyzing multiple relationships between blocks of variables. Under this framework, we establish the relationships among the blocks by taking into account some previous knowledge (e.g. theory) of the phenomenon under analysis. In addition, we assume that each block of variables plays the role of a theoretical concept represented in the form of a latent (unobserved) variable.


#Chapter 2

First, imagine that you have a network of variables. Usually, the connections in the network are assumed to represent some cause-effect process. But you can think of it as a flow chart where the process flows in one direction (no loops allowed). Having defined the network, one of the goals is to quantify the connections or relationships among the variables.

In its purest data analysis sense, PLS can be viewed as a set of methods for analyzing multiple relationships between various blocks of variables (or tables of data). If we consider a block of variables as a data table, the dierence between the various PLS methods depends on the number of tables and the type of relationships between variables.

Although the essence of latent variables is that they cannot be directly measured, that does not mean they are nonsense or useless. To make them operative, latent variables are indirectly measured by means of variables which can be perfectly observed-measured. These types of
variables are called manifest variables (MVs), also known as indicators or items. We assume that manifest variables contain information that reflect or indicate one aspect of the construct; hence we use the information contained in indicators to obtain an approximate representation of the latent variable.

In the first case, called reflective way, manifest variables are considered as being caused by the latent variables. The second case is known as formative way because the latent construct is supposed to be formed by its indicators.

```{r}
data("spainfoot")

# rows of the inner model matrix
Attack = c(0, 0, 0)
Defense = c(0, 0, 0)
Success = c(1, 1, 0)
# path matrix created by row binding
foot_path = rbind(Attack, Defense, Success)
# add column names (optional)
colnames(foot_path) = rownames(foot_path)

# plot the path matrix
innerplot(foot_path)

# define list of indicators: what variables are associated with
# what latent variables
foot_blocks = list(1:4, 5:8, 9:12)

# Success in formative mode B
foot_modes = c("A", "A", "A")

foot_pls = splspm(spainfoot, foot_path, foot_blocks, modes = foot_modes)

# plotting loadings of the outer model
plot(foot_pls, what = "loadings", arr.width = 0.1)

```

# Chapter 3

To assess how closely a PLS model \ts" the data, we use prediction error as our measure of prediction accuracy, and resampling methods for inference purposes.

From the Structural Equation Modeling (SEM) standpoint, PLS-PM offers a different approach that doesn't impose any distributional assumptions on the data that are hard to meet in real life, especially for non-experimental data. When we use a covariance-based SEM approach we are implicitly assuming that the data is generated by some true theoreticalmodel. In this scenario, the goal of covariance structure analysis (CSA) is to recover the "true" model that gave rise to the observed covariances. Briey, when using CSA we are concerned with fitting a model and reproducing the observed covariances. This approach resorts to classical theory of statistical inference and is based on a heavy use of distributional assumptions about the behavior and personality of the data. Consequently, the analyst is forced to move slowly; and the modeling process requires careful thought and stringent justifications that more often than not end up compromising the whole analysis with the bizarre (and sometimes contradictory) principle of the data must follow the model.

In contrast, PLS-PM does not rely on a data-generation process and causal-modeling interpretations. Instead, PLS-PM treats the data "just" as a dataset. What I mean by this is that although there can be a data-generation process in principle, it plays no direct role in PLS-PM. The proposed models are not considered to be ground truth, but only an approximation with useful predictiveness. In other words, PLS-PM assumes no model by which the data were generated. There is only the data and nothing but the data. In this sense, PLS-PM follows the spirit of a dimension reduction technique that we can use to get useful insight of the data on hand. The ultimate goal in PLS-PM is to provide a practical summary of how the set of dependent variables are systematically explained by their sets of predictors.

Let's assume that we have p variables measured on n observations (i.e. individuals, cases, samples), and that the variables can be divided in J blocks. We will use the following notation in the rest of the chapter:


- $\boldsymbol{X}$ is the data set containing the *n* observations and *p* variables. 
You can think of $\boldsymbol{X}$ as a matrix of dimension *nxp*

- $\boldsymbol{X}$ can be divided in *J* (mutually exclusive) blocks $\boldsymbol{X_1;X_2,...,X_J}$

- Each block $\boldsymbol{X}$ has *K* variables: $\boldsymbol{X_j1;X_j2,...,X_jK}$

Each block $\boldsymbol{X_j}$ is assumed to be associated with a latent variable $LV_j$.

The estimation of a latent variable, also known as score, is denoted by $\hat{LV_j} = Y$


**Inner relationships**

- Linear Relationships
    
    $LV_j = \beta_{0}+\sum\limits_{i->j}{\beta_{jk}LV_{i}}+error_j$
    
    The subscript *i* of $LV_i$ refers to all the latent variables that are supposed to predict $LV_j$.
    The coefficients $\beta_{ji}$ are the path coefficients and they represent the "strength and direction" 
    of the relations between the response $LV_j$ and the predictors $LV_i$. $\beta_{0}$ is just the intercept term,
    and the $error_j$ term accounts for the residuals.


- Recursive Models
- Regression Specification

    $E(LV_j|LV_i) = \lambda_{0j}+\sum\limits_{i->j}\lambda_{ji}LV_{i}$

    we want to understand as far as possible the conditional expected values of the 
    response $LV_j$ determined by its predictors $LV_i$. 
    The only extra assumption is:
    
    $cov(LV_j,error_j)=0$
    
**Outer relationships**

Linear relationships

* Reflective (MVs reflects LV, MV <- LV, mode B) 
    - $X_jk = \lambda_{ojk}+\lambda_{jk}LV_j+error_{jk}$
* Formative (Mvs form LV, MV -> LV, mode A)
    - $LV_j = \lambda_{0j}+\lambda_{jk}X_{jk}+error_j$
    
Regression specification    

* Reflective
    - $E(X_jk|LV_j) = \lambda_{ojk}+\lambda_{jk}LV_j$
* Formative (Mvs form LV, MV -> LV, mode A)
    - $E(LV_j|X_{jk}) = \lambda_{0j}+\lambda_{jk}X_{jk}$

The weights relations
$$\hat{LV_j} = Y_i = \sum\limits_{k}w{jk}X_{jk}$$

### PLS-PM Algorithm Overview

- Stage 1: Get the weights to compute latent variable scores
- Stage 2: Estimating the path coefficients (inner model)
- Stage 3: Obtaining the loadings (outer model)

#### Stage 1: Iterative Process

    Start: Initial arbitrary outer weights (normalized to obtain standardized LVs)
    Step 1: Compute the external approximation of latent variables
    Step 2: Obtain inner weights
    Step 3: Compute the internal approximation of latent variables
    Step 4: Calculate new outer weights
    Repeat step 1 to step 4 until convergence of outer weights

**Step 0: Initial arbitrary outer weights**

$$\hat{w}_k = \hat{w}_{jk} = 1$$

**Step 1: External estimation**

$$Y_k \propto X_k\hat{w}_k$$

The proportional symbol $\propto$ indicates that each score $Y_j$ depends on its manifest variables $X_{jk}$ although there's something missing. To be precise with the formula of the scores we need to use the following equation:

$$Y_k =\pm f  \sum\limits_{k}\hat{w}_{jk}X_{jk}$$

where $\pm f$ is $sign\big[\sum\limits_{k}sign\{cor(X_{jk},Y_j)\}\big]$.

The standardized LVs are finally expressed as:

$Y_k = \sum\limits_{k}w_{jk}X_{jk}$

**Step 2: Obtain Inner weights**

The goal in this step is to re-calculate scores but this time in a different way. Instead of getting the score of a latent varibale as a linear combination of its indicators, we will get a score as the linear combination of its associated latent variables. In other words, the
connections among constructs in the inner model are taken into account in order to obtain a proxy of each latent variable calculated this time as a weighted aggregate of its adjacent
LVs.

The internal estimation of $LV_j$ denoted by $Z_j$ is defined by:

$$Z_j = \sum\limits_{i \leftrightarrow j} e_{ij}Y_i$$

where $\leftrightarrow$ means that $LV_j$ is associated or connected with $LV_i$.

Because now we are dealing with the inner model, the weights $e_{ij}$ for this special combination are called inner weights.

*Factor scheme:*

$$
e_{ij} \begin{cases} cor(Y_j,Y_i) LV_j,LV_j & \space adjacents,\\  0 & otherwise. \end{cases}
$$

*Path scheme:*

In this case the LVs are divided in antecedents (predictors) and followers (predictands) depending on the cause-effects relationships between two LVs. An LV can be either a follower, if it is caused by another LV, or an antecedent if it is the cause of another LV. If $LV_i$ is a follower of $LV_j$ then the inner weight is equal to the correlation between $Y_i$ and $Y_j$. On the other hand, for the antecedents $LV_i$ of $LV_j$ the inner weights are the regression coefficient of $Y_i$ in the multiple regression of $LV_j$ on the $LV_i$'s associated to the antecedents of $LV_j$.

**Step 3: Internal Approximation**

Once we have the inner weights, we compute the internal estimation $Z_j$ as:

$$Z_j = \sum\limits_{i \leftrightarrow j}e_{ij}Y_i$$

**Step 4: Updating Outer weights**

Once the inside approximation is done, the internal estimates $Z_j$ must then be considered with regard their indicators. This is done by updating the outer weights. There are basically two ways of calculating the outer weights $w_{ji}$: (1) mode A, and (2) mode B.

**Mode A**

In reflective blocks (mode A) we obtain the outer weights $\hat{w}_{jk}$ with simple regressions of each indicator $X_{j1},X_{j2},...,X_{jk}$ on their latent score $Y_j$

$$
\hat{w}_{jk} = (Y_j^{'}Y_j)^{-1}Y_j^{'}X_{jk}
$$
**Mode B**
In formative blocks (mode B) we obtain the vector of outer weights $\hat{w}_{jk}$ with a multiple regression of $Y_j$ on $\boldsymbol X_j$

$$
\boldsymbol{\hat{w}_{jk}} = \boldsymbol{(X_j^{'}X_{j})^{-1}X_j^{'}}Y_j
$$
Check for convergence by comparing outer weights from last iteration.

#### Stage 2: Path Coefficients

The second stage of the algorithm consists of calculating the path coecient estimates,
$\hat{\beta_{ji}} = B_{ji} $


$$
B_{ji} = (Y_i^{'}Y_i)^{-1}Y_i^{'}Y_j
$$

#### Stage 3: Loadings

$$
\hat{\lambda_{ji}} = cor(X_{jk},Y_k)
$$


# Chapter 4

```{r}
data(spainfoot)

# rows of the path matrix
Attack = c(0, 0, 0)
Defense = c(0, 0, 0)
Success = c(1, 1, 0)
# creating the matrix by binding rows
foot_path = rbind(Attack, Defense, Success)
# add column names (optional)
colnames(foot_path) = rownames(foot_path)

# list indicating what variables are associated with what latent
# variables
foot_blocks = list(1:4, 5:8, 9:12)

# all latent variables are measured in a reflective way
foot_modes = rep("A", 3)

# run plspm analysis
foot_pls = splspm(spainfoot, foot_path, foot_blocks, modes = foot_modes)



```

##Measurement Model Assessment:Reflective Indicators

On one hand, reflective indicators need to have strong mutual association. In other words, they need to have strong ties. If one of them goes up, the rest will also increase their values. If one of them goes down, the rest will decrease their values too. Shortly, they will be highly correlated. On the other hand, refl ective indicators need to get along with its latent variable; they must show sings of membership and belonging to one and only one latent variable: they need to be loyal to its construct. If one indicator loads higher on another construct, this could be evidence of treason.

Basically, we must evaluate three aspects of reflective measures:
- Unidimensionality of the indicators
- Check that indicators are well explained by its latent variable
- Assess the degree to which a given construct is different from other constructs

**Unidimensionality of the indicators**
If you have a bunch of variables that are supposed to be measuring some aspect of the same thing (the same latent variable) you would expect those variables to roughly point in the same direction. This is what unidimensionality implies.

The refl ective indicators must be in a space of one dimension since they 
are practically indicating the same latent variable.
In PLS-PM we have three main indices to check unidimensionality:
- Calculate the Cronbach's alpha
- Calculate the Dillon-Goldstein's rho
- Check the first eigenvalue of the indicators' correlation matrix

```{r}
# unidimensionality
foot_pls$unidim
```

*Cronbach's alpha*

The Cronbach's alpha is a coefficient that is intended to evaluate how well a block of indicators measure their corresponding latent construct. You can think of it as an **average inter-variable correlation** between indicators of a refl ective construct. If a block of manifest variables is unidimensional, they have to be highly correlated, and consequently we expect them to have a high average inter-variable correlation. It is important to keep in mind that the computation of the Cronbach's alpha requires the observed variables to be standardized and positively correlated.

```{r}
# cronbach's alpha
foot_pls$unidim[, 3, drop = FALSE]
```

As a rule of thumb, a cronbach's alpha greater than 0.7 is considered acceptable.

*Dillon-Goldstein's rho*

Focuses on the variance of the sum of variables in the block of interest.

```{r}

# dillon-goldstein rho
foot_pls$unidim[, 4, drop = FALSE]

```

*First eigenvalue*

The third metric involves an eigen-analysis of the correlation matrix of each set of indicators. The use of this metric is based on the importance of the first eigenvalue. If a block is unidimensional, then the rst eigenvalue should be much more" larger than 1 whereas the
second eigenvalue should be smaller than 1.

```{r}
# eigenvalues
foot_pls$unidim[, 5:6]

```


**4.3.3 Loadings and Communalities**

```{r}
# loadings and communalities
foot_pls$outer_model
```
The first column corresponds to the block which is codified as a factor (i.e. categorical variable). The second column contains the outer weights. The third column are the loadings (correlations). **Loadings greater than 0.7 are acceptable. To see why they are acceptable we use the communalities in the fourth column. Communalities are just squared loadings. They represent the amount of variablity explained by a latent variable. A loading grater than 0.7 means that more than $0.7^2 = 50%$ of the variablity in an indicator is captured by its latent construct.** The fifth column are the redundancies; we'll talk about them later
in this chapter.

**Communality**

Communality is calculated with the purpose to check that indicators in a block are well explained by its latent variable. Communalities are simply squared loadings and they measure the part of the variance between a latent variable and its indicator that is common to both. To see why, we need to assume that each indicator represents an error measurement of its construct. The relation:

$$
mv_{jk} = loading_{jk}LV_j + error_{jk}
$$
implies that the latent variable $LV_j$ explains its indicator $mv_{jk}$, so we have to evaluate how well the indicators are explained by its latent variable. To do this, we examine the loadings which indicate the amount of variance shared between the construct and its indicators. The communality for the $jk$-th manifest variable of the $j$-th block is calculated as:

$$
Com(LV_j, mv_{jk}) = cor^2(LV_j,mv_{jk}) = loading_{jk}^2
$$
Looking at the previous formula, communality measures how much of a given manifest variable's variance is reproducible from the latent variable. In other words, the part of variance between a construct and its indicators that is common to both. One expects to have more
shared variance between an LV and its mv than error variance, that is:

$$
loading_{jk}^2 > var(error_{jk})
$$

**Cross loadings**

Besides checking the loadings of the indicators with their own latent variables, we must also check the so-called cross-loadings. That is, the loadings of an indicator with the rest of latent variables. The reason for doing so is that we need to be sure that we don't have traitor indicators.

```{r, echo=FALSE}
# cross-loadings
foot_pls$crossloadings

library(reshape)
# reshape crossloadings data.frame for ggplot
xloads = melt(foot_pls$crossloadings, id.vars = c("name", "block"),
variable_name = "LV")

library(ggplot2)
# bar-charts of crossloadings by block
ggplot(data = xloads,
aes(x = name, y = value, fill = block)) +
# add horizontal reference lines
geom_hline(yintercept = 0, color = "gray75") +
geom_hline(yintercept = 0.5, color = "gray70", linetype = 2) +
# indicate the use of car-charts
geom_bar(stat = 
'identity'
, position = 'dodge'
) +
# panel display (i.e. faceting)
facet_wrap(block ~ LV) +
# tweaking some grahical elements
theme(axis.text.x = element_text(angle = 90),
line = element_blank(),
plot.title = element_text(size = 12)) +
# add title
ggtitle("Crossloadings")
```

Keep in mind that cross-loadings allow us to we evaluate the extent to which a given construct differentiates from the others. The whole idea is to verify that the shared variance between a construct and its indicators is larger than the shared variance with other constructs. In other words, no indicator should load higher on another construct than it does on the construct it intends to measure. Otherwise, it is a traitor indicator.

## Structural Model Assessment

```{r}
# inner model
foot_pls$inner_model
```



# Glossary

- Partial Least Squares Path Modeling (PLS-PM)
- Structural Equation Modeling (SEM)
- Latent Variable (LM) 
- Manifest Variable (MV)
- $\beta$ (path coefficient)
- $\hat{\beta_{ji}} = B_{ji}$ (estimated path coefficients)
- $LV_j = \beta_{0}+\sum\limits_{i->j}{\beta_{jk}LV_{i}}+error_j$ (inner relationship)
- $\lambda$ (loadings, final? outer weights)
- $w$ (outer weights)
- $X_jk = \lambda_{ojk}+\lambda_{jk}LV_j+error_{jk}$ (reflective outer relationship, (Mvs form LV, MV -> LV, mode A))
- $LV_j = \lambda_{0j}+\lambda_{jk}X_{jk}+error_j$ (fo rmative outer relationship, mode B)
- $\hat{LV_j} = Y_i = \sum\limits_{k}w{jk}X_{jk}$ (score, predicted LV)
- $Y_i$ (score, predicted LV)
- $Z_j$ (internal estimation of $LV_j$)
- $e_{ij}$ (inner weights)
- communality = $loading^2$


# Help

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)

```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
